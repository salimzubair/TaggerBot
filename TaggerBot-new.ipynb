{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start tika server. The Tika Server is the Parser\n",
    "#!java -jar \"C:\\Users\\szubair\\AppData\\Local\\Continuum\\anaconda3\\tika-server-1.22.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import os\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.max_length = 10000000\n",
    "from datetime import datetime\n",
    "from dateparser.search import search_dates\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from summa.summarizer import summarize\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe\"\n",
    "from pdf2image import convert_from_path, convert_from_bytes\n",
    "from pdf2image.exceptions import (\n",
    "    PDFInfoNotInstalledError,\n",
    "    PDFPageCountError,\n",
    "    PDFSyntaxError\n",
    ")\n",
    "import dask.dataframe as dd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a parameter for tika parsers. This declaration solves the status 422 server error\n",
    "headers = {'X-Tika-PDFextractInlineImages': 'true', \"X-Tika-OCRLanguage\": \"eng\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing all Lookup Data\"\"\"\n",
    "\n",
    "#import project lookup data\n",
    "proj_lookup = pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\proj_codes.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "#import circuit lookup table and convert to list\n",
    "circuit_lookup = list(pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\circuits.csv\", header=None, encoding = \"ISO-8859-1\")[0])\n",
    "\n",
    "#import station lookups\n",
    "code_lookup = list(pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\station-codes.csv\", header = None)[0])\n",
    "code_name_lookup = list(pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\station-names.csv\", encoding = \"ISO-8859-1\")['Code'])\n",
    "name_lookup = list(pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\station-names.csv\", encoding = \"ISO-8859-1\")['Name'])\n",
    "\n",
    "\n",
    "#import reservoirs lookup and convert to lists\n",
    "reservoir_name = list(pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\reservoirs.csv\", encoding = \"ISO-8859-1\", header = None)[0])\n",
    "reservoir_values = list(pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\reservoirs.csv\", encoding = \"ISO-8859-1\", header = None)[1])\n",
    "\n",
    "#import keywords list\n",
    "keywords_lookup = list(pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\keywords.csv\", encoding = \"ISO-8859-1\", header = None)[0])\n",
    "\n",
    "#import authors list\n",
    "authors_lookup = list(pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\authors.csv\", encoding = \"ISO-8859-1\", header = None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse text from scanned files\n",
    "def ocr_pdf(file):\n",
    "    images = convert_from_path(file)\n",
    "    ocr_list = [pytesseract.image_to_string(x) for x in images]\n",
    "    ocr = ''\n",
    "    return ocr.join(ocr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RETURN THE ROOT FOLDER BY EXTRACTING MID STRING\"\"\"\n",
    "def find_root(path):\n",
    "    return path[32: path.find(\"\\\\\", 32)] #the number here is 32 because raw strings. With regular strings the number is 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to extract dates from a piece of text\"\"\"\n",
    "today = datetime.today()\n",
    "\n",
    "#Match strings that contain only numbers and special characters\n",
    "def special_match(strg, search=re.compile(r'[^|\\&+\\-%*/=!>0-9.]').search):\n",
    "    return not bool(search(strg))\n",
    "\n",
    "def find_dates(text):\n",
    "    found_dates = search_dates(text, languages = ['en'], settings={'STRICT_PARSING': True})\n",
    "    \"\"\"Exclude Possible error matches\"\"\"\n",
    "    found_dates = [x for x in found_dates if x[1] <= datetime.today() and x[1] > datetime(1940, 1, 1)] #only return dates between 1940 and today\n",
    "    found_dates = [x for x in found_dates if re.fullmatch('\\d{4}', x[0]) is None]  #search_dates sometimes confuses years for full dates. This excludes those errors\n",
    "    found_dates = [x for x in found_dates if x[1].hour == 0] #exclude matches that includes hours, mins, secs. Most likely error match\n",
    "    return found_dates[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all the file paths in folder and return as numpy array\n",
    "def return_paths(folder):\n",
    "    paths = np.array([os.path.join(r, file) for r, d, f in os.walk(folder) for file in f])\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Tika Parser on Texts\n",
    "def return_parsed(paths):\n",
    "    try:\n",
    "        return parser.from_file(paths, headers=headers)\n",
    "    except:\n",
    "        return 'path error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract texts\n",
    "def return_texts(parsed, paths):\n",
    "    if 'content' in parsed and parsed['content'] is not None:\n",
    "        return parsed['content'] #extract 'content' from parsed texts\n",
    "    else:\n",
    "        try:\n",
    "            return ocr_pdf(paths) #if no 'content' from tika parser, try OCRing the document\n",
    "        except:\n",
    "            return \"no content\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zip Paths and Texts\n",
    "def return_zip_items(paths, texts):\n",
    "    return ''.join([paths, texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return Metadata values of document from tika parser\n",
    "def return_metadata(parsed):\n",
    "    try:\n",
    "        return parsed['metadata']\n",
    "    except:\n",
    "        return \"no content\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return document filename\n",
    "def return_filenames(paths):\n",
    "    try:\n",
    "        return os.path.basename(paths)\n",
    "    except:\n",
    "        return \"no content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return document datae\n",
    "def return_dates(filenames, texts, metadata):\n",
    "    try:\n",
    "        return find_dates(filenames) #first find date in document filename\n",
    "    except:\n",
    "        try:\n",
    "            return find_dates(texts[:1000]) #if no date in filename, search 1st 1000 characters for date\n",
    "        except:\n",
    "            try:\n",
    "                return metadata['Last-Modified'] # if no date in 1st 1000 characters, returned date last modified\n",
    "            except:\n",
    "                return \"no content\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract Keywords from text\"\"\"\n",
    "def return_keywords(zip_items):\n",
    "    try:\n",
    "        keywords_list = [keyword for keyword in keywords_lookup if keyword in zip_items.lower()]\n",
    "        return list(dict.fromkeys(keywords_list))\n",
    "    except:\n",
    "        return 'no content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the document author\n",
    "def return_authors(paths, zip_items):\n",
    "    authors_list = []\n",
    "    try:\n",
    "        authors_list = [author.title() for author in authors_lookup if author in zip_items[:1000].lower()]\n",
    "        if len(authors_list) == 0:\n",
    "            authors_list.append(find_root(repr(paths)))\n",
    "        return list(dict.fromkeys(authors_list))\n",
    "    except:\n",
    "        return 'no content'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use gensim summarize module to summarize the document\n",
    "def return_descriptions(texts):\n",
    "    try:\n",
    "        return summarize(texts, words=50, language='english')\n",
    "    except:\n",
    "        return 'no content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search Paths and Texts for Project IDs referenced\n",
    "def return_proj_ids(zip_items):\n",
    "    #populate proj_codes list with all project IDs found\n",
    "    proj_no = []\n",
    "    proj_id_regex = r\"[BDEFGTYbdefgty][ABCEFILMPRSVYZabcefilmprsvyz][-\\s]?\\d{4,5}|[A-Z]{3}MON[-\\s]?\\d{1,2}[A-Za-z]?|[A-Z]{3}WORKS[-\\s]?\\d{1,2}[A-Za-z]?\"\n",
    "    proj_no.append(re.findall(proj_id_regex , zip_items[:1000]))\n",
    "\n",
    "    #remove duplicate occurrences\n",
    "    proj_codes = [list(dict.fromkeys(x)) for x in proj_no]\n",
    "        \n",
    "    #reformat wrongly formatted project codes to standard\n",
    "    regex1 = r\"([A-Z]{3}MON)\\s?(\\d{1,2}[A-Za-z]?)\"           #ABFMON 02(A)/ ABFMON02(A) ----> ABFMON-02(A)\n",
    "    regex2 = r\"([A-Z]{3}WORKS)\\s?(\\d{1,2}[A-Za-z]?)\"         #BRGWORKS 01(A)/BRGWORKS01(A)  ---->  BRGWORKS-01(A)\n",
    "    regex3 = r\"([A-Z]{3}MON)-(\\d{1}[A-Za-z]?)\"               #ABFMON-2/ CLBMON-1A  ---->  ABFMON-02(A)\n",
    "    regex4 = r\"([A-Z]{3}WORKS)-(\\d{1}[A-Za-z]?)\"             #BRGWORKS-1/ CLBWORKS-2A  ---->  BRGWORKS-01(A)\n",
    "    regex5 = r\"([BDEFGTYbdefgty][ABCEFILMPRSVYZabcefilmprsvyz])\\s?(\\d{4,5})\"   #TY7111 / TY 7111  ---->  TY-7111\n",
    "\n",
    "    #capture rightly formatted project codes too\n",
    "    regex6 = r\"[A-Z]{3}MON-\\d{2}[A-Za-z]?\"\n",
    "    regex7 = r\"[A-Z]{3}WORKS-\\d{2}[A-Za-z]?\"\n",
    "    regex8 = r\"[A-Za-z]{2}-\\d{4}\"\n",
    "\n",
    "    new_proj = []\n",
    "    for reg in proj_codes:\n",
    "        a = [re.sub(regex1, r\"\\1-\\2\", x) for x in reg]\n",
    "        b = [re.sub(regex2, r\"\\1-\\2\", x) for x in reg] + a\n",
    "        c = [re.sub(regex3, r\"\\1-0\\2\", x) for x in b + reg] + b\n",
    "        d = [re.sub(regex4, r\"\\1-0\\2\", x) for x in c + reg] + c\n",
    "        e = [re.sub(regex5, r\"\\1-\\2\", x) for x in reg] + d\n",
    "        e.append(re.findall(regex6 , str(reg)))\n",
    "        e.append(re.findall(regex7 , str(reg)))\n",
    "        e.append(re.findall(regex8 , str(reg)))\n",
    "        new_proj.append(e)\n",
    "    proj_codes = new_proj\n",
    "    \n",
    "    #validate project codes are in project codes lookup table\n",
    "    validate_codes = []\n",
    "    for x in proj_codes:\n",
    "        y = [str(z).upper() for z in x if str(z).upper() in list(proj_lookup['Project Code'])]\n",
    "        validate_codes.append(y)\n",
    "    proj_codes = []\n",
    "\n",
    "    #remove duplicate occurrences      \n",
    "    for x in validate_codes:\n",
    "        proj_codes = list(dict.fromkeys(x))\n",
    "    return proj_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use proj_lookup to find project name for all project codes\n",
    "def return_proj_names(proj_ids):\n",
    "    try:\n",
    "        for x in proj_ids:\n",
    "            proj_names = [list(proj_lookup['Project Name'])[list(proj_lookup['Project Code']).index(y)] for y in proj_ids]\n",
    "        return proj_names\n",
    "    except:\n",
    "        return \"no content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find circuits referenced in paths and texts\n",
    "def return_circuits(zip_items):\n",
    "    try:\n",
    "        item_delimited = re.split('[|\\^&+\\-%*/=!>\\s\\-.,\\\\\\\\]', zip_items)     #delimit because some circuits are contained in other circuits e.g. 1L1 AND 1L135 \n",
    "        circuit_list = [circuit for circuit in circuit_lookup if circuit in item_delimited]\n",
    "        return circuit_list\n",
    "    except:\n",
    "        return 'no content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find station names referenced in paths and texts\n",
    "def return_stations(zip_items):\n",
    "    name_list = []\n",
    "    name_list.append([code_name_lookup[name_lookup.index(name)] for name in name_lookup if name in zip_items.lower()])\n",
    "\n",
    "    #remove duplicate occurrences\n",
    "    for x in name_list:\n",
    "        stations = list(dict.fromkeys(x))\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find reservoirs referenced in paths and texts\n",
    "def return_reservoirs(zip_items):\n",
    "    reservoirs = []\n",
    "    reservoirs = [reservoir_name[reservoir_values.index(value)] for value in reservoir_values if value in zip_items.lower()]\n",
    "    return reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return prediction probability \n",
    "def prediction_probability(encoded, probas):\n",
    "    return probas[encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import necessary materials for Arch Data Classification\"\"\"\n",
    "#import arch training data\n",
    "arch_df = pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\parsed-arch_data-training-data-2.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Create Vectorizer Object for Arch Classification\n",
    "arch_tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "#Fit Arch Data Classifier Vectorizer\n",
    "arch_y, arch_mappings =arch_df.Arch_Data.factorize()\n",
    "arch_tfidf.fit_transform(arch_df.Texts).toarray()\n",
    "\n",
    "#Import Arch Data Classifier Model\n",
    "arch_clf = pickle.load(open(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\final-arch-model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import necessary materials for Document Type Classification\"\"\"\n",
    "#import Document Type training data\n",
    "doc_df = pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\parsed-doc-type-training-data-4.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Create Vectorizer Object for Document Type Classification\n",
    "doc_tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "#Fit Document Type Classifier Vectorizer\n",
    "doc_y, doc_mappings =doc_df.Document_Type.factorize()\n",
    "doc_tfidf.fit_transform(doc_df.Texts).toarray()\n",
    "\n",
    "#mport Document Type Classifier Model\n",
    "doc_clf = pickle.load(open(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\final-doc-type-model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import necessary materials for Subject Area Classification\"\"\"\n",
    "#import Document Type training data\n",
    "sub_df = pd.read_csv(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\parsed-subject-training-data-5.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Create Vectorizer Object for Subject Area Classification\n",
    "sub_tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "#Fit Subject Area Classifier Vectorizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "sub_y = [set(i.split(',')) for i in sub_df.Subject_Area]\n",
    "sub_y = mlb.fit_transform(sub_y)\n",
    "sub_tfidf.fit_transform(sub_df.Texts).toarray()\n",
    "\n",
    "#Import Document Type Classifier Model\n",
    "sub_clf = pickle.load(open(r\"\\\\esri-shelf\\ESRIENVIROHUB\\TaggerBot\\subject-model-2.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default Unclassified Subject Areas to the string \"Environment\"\n",
    "def default_subject(Subject):\n",
    "    if Subject == ():\n",
    "        return 'Environment'\n",
    "    else:\n",
    "        return Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(inputFolder):\n",
    "    \n",
    "    paths = return_paths(inputFolder)\n",
    "    tagger_df = pd.DataFrame({'Paths': paths})\n",
    "    tagger_df = dd.from_pandas(tagger_df, npartitions=5) #Convert Pandas DataFrame to Dask DataFrame\n",
    "    \n",
    "    parsed = tagger_df.apply(lambda row: return_parsed(row['Paths']), axis = 1)\n",
    "    tagger_df['Parsed'] = parsed \n",
    "    \n",
    "    texts =  tagger_df.apply(lambda row: return_texts(row['Parsed'], row['Paths']), axis = 1)\n",
    "    tagger_df['Texts'] = texts\n",
    "    \n",
    "    zip_items =  tagger_df.apply(lambda row: return_zip_items(row['Paths'], row['Texts']), axis = 1)\n",
    "    tagger_df['Zip Items'] = zip_items\n",
    "                                                          \n",
    "    metadata =  tagger_df.apply(lambda row: return_metadata(row['Parsed']), axis = 1)\n",
    "    tagger_df['Metadata'] = metadata\n",
    "    os.system(r'taskkill /IM \"java.exe\" /F') #end tika process after tagging\n",
    "    \n",
    "    filenames =  tagger_df.apply(lambda row: return_filenames(row['Paths']), axis = 1)\n",
    "    tagger_df['Filenames'] = filenames\n",
    "    \n",
    "    dates =  tagger_df.apply(lambda row: return_dates(row['Filenames'], row['Texts'], row['Metadata']), axis = 1)\n",
    "    tagger_df['Dates'] = dates\n",
    "    \n",
    "    keywords =  tagger_df.apply(lambda row: return_keywords(row['Zip Items']), axis = 1)\n",
    "    tagger_df['Keywords'] = keywords\n",
    "    \n",
    "    authors =  tagger_df.apply(lambda row: return_authors(row['Paths'], row['Zip Items']), axis = 1)\n",
    "    tagger_df['Authors'] = authors\n",
    "    \n",
    "    #descriptions =  tagger_df.apply(lambda row: return_descriptions(row['Texts']), axis = 1)\n",
    "    #tagger_df['Descriptions'] = descriptions\n",
    "    \n",
    "    proj_ids =  tagger_df.apply(lambda row: return_proj_ids(row['Zip Items']), axis = 1)\n",
    "    tagger_df['Project IDs'] = proj_ids\n",
    "    \n",
    "    proj_names =  tagger_df.apply(lambda row: return_proj_names(row['Project IDs']), axis = 1)\n",
    "    tagger_df['Project Names'] = proj_names\n",
    "    \n",
    "    circuits =  tagger_df.apply(lambda row: return_circuits(row['Zip Items']), axis = 1)\n",
    "    tagger_df['Circuits'] = circuits\n",
    "    \n",
    "    stations =  tagger_df.apply(lambda row: return_stations(row['Zip Items']), axis = 1)\n",
    "    tagger_df['Stations'] = stations\n",
    "    \n",
    "    reservoirs =  tagger_df.apply(lambda row: return_reservoirs(row['Zip Items']), axis = 1)\n",
    "    tagger_df['Reservoirs'] = reservoirs\n",
    "    tagger_df = tagger_df.compute() #Convert Dask DataFrame back to Pandas DataFrame #Exploring faster options\n",
    "\n",
    "    arch_X = arch_tfidf.transform(tagger_df.Texts).toarray()\n",
    "    tagger_df['Arch_Data'] = arch_mappings[arch_clf.predict(arch_X)]\n",
    "    \n",
    "    doc_X = doc_tfidf.transform(tagger_df.Texts).toarray()\n",
    "    tagger_df['encoded_pred'] = doc_clf.predict(doc_X)\n",
    "    tagger_df['Document_Type'] = doc_mappings[tagger_df['encoded_pred']]\n",
    "    tagger_df['pred_probas'] = list(doc_clf.predict_proba(doc_X))\n",
    "    tagger_df['Document_Type_Probability'] =  tagger_df.apply(lambda row: prediction_probability(row['encoded_pred'], row['pred_probas']), axis = 1)\n",
    "    \n",
    "    sub_X = sub_tfidf.transform(tagger_df.Texts).toarray()\n",
    "    tagger_df['Subject_Area'] = mlb.inverse_transform(sub_clf.predict(sub_X))\n",
    "    tagger_df['Subject_Area'] = tagger_df.apply(lambda row: default_subject(row['Subject_Area']), axis = 1)\n",
    "    \n",
    "    return tagger_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-24 09:58:07,518 [Thread-12   ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2020-02-24 09:58:07,528 [Thread-11   ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2020-02-24 09:58:07,555 [Thread-10   ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2020-02-24 09:58:07,556 [Thread-13   ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2020-02-24 09:58:12,548 [Thread-12   ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2020-02-24 09:58:12,554 [Thread-11   ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2020-02-24 09:58:12,575 [Thread-10   ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2020-02-24 09:58:12,592 [Thread-13   ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paths</th>\n",
       "      <th>Parsed</th>\n",
       "      <th>Texts</th>\n",
       "      <th>Zip Items</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Filenames</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Project IDs</th>\n",
       "      <th>Project Names</th>\n",
       "      <th>Circuits</th>\n",
       "      <th>Stations</th>\n",
       "      <th>Reservoirs</th>\n",
       "      <th>Arch_Data</th>\n",
       "      <th>encoded_pred</th>\n",
       "      <th>Document_Type</th>\n",
       "      <th>pred_probas</th>\n",
       "      <th>Document_Type_Probability</th>\n",
       "      <th>Subject_Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\1537874-004-R...</td>\n",
       "      <td>{'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\1537874-004-R...</td>\n",
       "      <td>{'Author': 'cmcallister', 'Content-Type': ['ap...</td>\n",
       "      <td>1537874-004-R-Rev0-LCR indexing 2017 Report 15...</td>\n",
       "      <td>2019-01-15 00:00:00</td>\n",
       "      <td>[agreement, als, amp, bass, bat, bathymetry, b...</td>\n",
       "      <td>[Golder Associates]</td>\n",
       "      <td>[CLBMON-45]</td>\n",
       "      <td>[CLBMON-45 LOW COL FISH POPULATION INDEXING]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ALH, BEC, BRX, CRO, RO2, TSS, WAX]</td>\n",
       "      <td>[Arrow Lakes Reservoir, Bear Creek Reservoir]</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>Reports</td>\n",
       "      <td>[0.9999998507448368, 1.1996440729850394e-08, 2...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(Fish and Aquatic,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\CLBMON-21 Yr1...</td>\n",
       "      <td>{'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\CLBMON-21 Yr1...</td>\n",
       "      <td>{'Content-Type': ['application/pdf', 'image/pn...</td>\n",
       "      <td>CLBMON-21 Yr12 2019-06-01.pdf</td>\n",
       "      <td>2019-06-01 00:00:00</td>\n",
       "      <td>[als, amec, amec foster wheeler, amp, applicat...</td>\n",
       "      <td>[test2]</td>\n",
       "      <td>[CLBMON-21]</td>\n",
       "      <td>[CLBMON-21 MID COL JUVENILE STURGEON DETECTION...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[LIK, REV, WHN]</td>\n",
       "      <td>[Arrow Lakes Reservoir, Arrow Lakes Reservoir,...</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>Reports</td>\n",
       "      <td>[0.999998256351634, 1.4587323123447914e-07, 4....</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>(Fish and Aquatic,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\CMSMON1a data...</td>\n",
       "      <td>{'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\CMSMON1a data...</td>\n",
       "      <td>{'Author': 'stephanie lingard', 'Content-Type'...</td>\n",
       "      <td>CMSMON1a data report 2018 field season_Novembe...</td>\n",
       "      <td>2018-11-13T23:44:55Z</td>\n",
       "      <td>[agreement, als, amp, bulletin, burn, char, ch...</td>\n",
       "      <td>[Instream Fisheries Research]</td>\n",
       "      <td>[CMSMON-01A]</td>\n",
       "      <td>[CMSMON-01A CHEAKAMUS RIVER JUVENILE OUTMIGRAN...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[BCR, CMS]</td>\n",
       "      <td>[Daisy Lake Reservoir (Shadow Lake)]</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>Reports</td>\n",
       "      <td>[0.9994142089623389, 4.1498967900233305e-05, 0...</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>(Fish and Aquatic,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\FINAL_Plan_fo...</td>\n",
       "      <td>{'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\FINAL_Plan_fo...</td>\n",
       "      <td>{'Author': 'Elinor McGrath', 'Company': 'Hewle...</td>\n",
       "      <td>FINAL_Plan_for_Fish_Passage_at_Wilsey_Dam_-_Ma...</td>\n",
       "      <td>2018-05-14 20:14:08</td>\n",
       "      <td>[acid rock drainage, advisory, agreement, als,...</td>\n",
       "      <td>[test2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>no content</td>\n",
       "      <td>[]</td>\n",
       "      <td>[LU2, RVS, SGR, WIL]</td>\n",
       "      <td>[Arrow Lakes Reservoir, Duncan Lake Reservoir,...</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>Reports</td>\n",
       "      <td>[0.9999998160530256, 6.25809278704018e-08, 5.4...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(Fish and Aquatic,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\Anadromous Sa...</td>\n",
       "      <td>{'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>\\\\esri-shelf\\ESRIENVIROHUB\\test2\\Anadromous Sa...</td>\n",
       "      <td>{'Author': 'Giles Shearing', 'Content-Type': [...</td>\n",
       "      <td>Anadromous Salmon Spawning Habitat Middle Shus...</td>\n",
       "      <td>2013-04-08 00:00:00</td>\n",
       "      <td>[agreement, als, amp, application, bat, bathym...</td>\n",
       "      <td>[test2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>no content</td>\n",
       "      <td>[]</td>\n",
       "      <td>[LU2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>Reports</td>\n",
       "      <td>[0.9999778668965223, 2.8360762965578354e-06, 4...</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>(Fish and Aquatic,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Paths  \\\n",
       "0  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\1537874-004-R...   \n",
       "1  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\CLBMON-21 Yr1...   \n",
       "2  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\CMSMON1a data...   \n",
       "3  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\FINAL_Plan_fo...   \n",
       "4  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\Anadromous Sa...   \n",
       "\n",
       "                                              Parsed  \\\n",
       "0  {'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...   \n",
       "1  {'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...   \n",
       "2  {'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...   \n",
       "3  {'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...   \n",
       "4  {'status': 200, 'content': '\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "...   \n",
       "\n",
       "                                               Texts  \\\n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "\n",
       "                                           Zip Items  \\\n",
       "0  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\1537874-004-R...   \n",
       "1  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\CLBMON-21 Yr1...   \n",
       "2  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\CMSMON1a data...   \n",
       "3  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\FINAL_Plan_fo...   \n",
       "4  \\\\esri-shelf\\ESRIENVIROHUB\\test2\\Anadromous Sa...   \n",
       "\n",
       "                                            Metadata  \\\n",
       "0  {'Author': 'cmcallister', 'Content-Type': ['ap...   \n",
       "1  {'Content-Type': ['application/pdf', 'image/pn...   \n",
       "2  {'Author': 'stephanie lingard', 'Content-Type'...   \n",
       "3  {'Author': 'Elinor McGrath', 'Company': 'Hewle...   \n",
       "4  {'Author': 'Giles Shearing', 'Content-Type': [...   \n",
       "\n",
       "                                           Filenames                 Dates  \\\n",
       "0  1537874-004-R-Rev0-LCR indexing 2017 Report 15...   2019-01-15 00:00:00   \n",
       "1                      CLBMON-21 Yr12 2019-06-01.pdf   2019-06-01 00:00:00   \n",
       "2  CMSMON1a data report 2018 field season_Novembe...  2018-11-13T23:44:55Z   \n",
       "3  FINAL_Plan_for_Fish_Passage_at_Wilsey_Dam_-_Ma...   2018-05-14 20:14:08   \n",
       "4  Anadromous Salmon Spawning Habitat Middle Shus...   2013-04-08 00:00:00   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [agreement, als, amp, bass, bat, bathymetry, b...   \n",
       "1  [als, amec, amec foster wheeler, amp, applicat...   \n",
       "2  [agreement, als, amp, bulletin, burn, char, ch...   \n",
       "3  [acid rock drainage, advisory, agreement, als,...   \n",
       "4  [agreement, als, amp, application, bat, bathym...   \n",
       "\n",
       "                         Authors   Project IDs  \\\n",
       "0            [Golder Associates]   [CLBMON-45]   \n",
       "1                        [test2]   [CLBMON-21]   \n",
       "2  [Instream Fisheries Research]  [CMSMON-01A]   \n",
       "3                        [test2]            []   \n",
       "4                        [test2]            []   \n",
       "\n",
       "                                       Project Names Circuits  \\\n",
       "0       [CLBMON-45 LOW COL FISH POPULATION INDEXING]       []   \n",
       "1  [CLBMON-21 MID COL JUVENILE STURGEON DETECTION...       []   \n",
       "2  [CMSMON-01A CHEAKAMUS RIVER JUVENILE OUTMIGRAN...       []   \n",
       "3                                         no content       []   \n",
       "4                                         no content       []   \n",
       "\n",
       "                              Stations  \\\n",
       "0  [ALH, BEC, BRX, CRO, RO2, TSS, WAX]   \n",
       "1                      [LIK, REV, WHN]   \n",
       "2                           [BCR, CMS]   \n",
       "3                 [LU2, RVS, SGR, WIL]   \n",
       "4                                [LU2]   \n",
       "\n",
       "                                          Reservoirs Arch_Data  encoded_pred  \\\n",
       "0      [Arrow Lakes Reservoir, Bear Creek Reservoir]         n             0   \n",
       "1  [Arrow Lakes Reservoir, Arrow Lakes Reservoir,...         n             0   \n",
       "2               [Daisy Lake Reservoir (Shadow Lake)]         n             0   \n",
       "3  [Arrow Lakes Reservoir, Duncan Lake Reservoir,...         n             0   \n",
       "4                                                 []         n             0   \n",
       "\n",
       "  Document_Type                                        pred_probas  \\\n",
       "0       Reports  [0.9999998507448368, 1.1996440729850394e-08, 2...   \n",
       "1       Reports  [0.999998256351634, 1.4587323123447914e-07, 4....   \n",
       "2       Reports  [0.9994142089623389, 4.1498967900233305e-05, 0...   \n",
       "3       Reports  [0.9999998160530256, 6.25809278704018e-08, 5.4...   \n",
       "4       Reports  [0.9999778668965223, 2.8360762965578354e-06, 4...   \n",
       "\n",
       "   Document_Type_Probability         Subject_Area  \n",
       "0                   1.000000  (Fish and Aquatic,)  \n",
       "1                   0.999998  (Fish and Aquatic,)  \n",
       "2                   0.999414  (Fish and Aquatic,)  \n",
       "3                   1.000000  (Fish and Aquatic,)  \n",
       "4                   0.999978  (Fish and Aquatic,)  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger(r\"\\\\esri-shelf\\ESRIENVIROHUB\\test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_experiment = [*range(1, 1001, 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concept for running TaggerBot in batches\n",
    "for x in list_experiment:\n",
    "    if list_experiment.index(x)+1 in list_experiment:\n",
    "        print([*range(x, list_experiment[list_experiment.index(x)+1])])\n",
    "    else:\n",
    "        print([*range(x, 1001)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
