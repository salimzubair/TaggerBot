# -*- coding: utf-8 -*-
"""subject-classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13sD5XlMs3z4zW2RwSQfq5KRwb1m8Rqs_
"""

#start tika server. The Tika Server is the Parser
!java -jar "path\to\tika-server-1.22.jar"

#import necessary modules
import os
import tika
tika.initVM()
from tika import parser
import os.path
import pandas as pd
import numpy as np
import spacy
import nltk
from nltk.corpus import stopwords
stopwords = set(stopwords.words('english'))
from spacy.lang.en import English
nlp = English()
nlp.max_length = 10000000
import nltk
from datetime import datetime
from dateparser.search import search_dates
from gensim.summarization import keywords
import warnings
warnings.filterwarnings("ignore")
from sklearn.feature_extraction.text import CountVectorizer
import re
import time
from collections import Counter
from summa.summarizer import summarize
import pytesseract
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files (x86)\Tesseract-OCR\tesseract.exe"
from pdf2image import convert_from_path, convert_from_bytes
from pdf2image.exceptions import (
    PDFInfoNotInstalledError,
    PDFPageCountError,
    PDFSyntaxError
)
import dask.dataframe as dd
import matplotlib.pyplot as plt

#define a parameter for tika parsers. This declaration solves the status 422 server error
headers = {'X-Tika-PDFextractInlineImages': 'true', "X-Tika-OCRLanguage": "eng"}

#import project data
main_df = pd.read_csv("consolidated-training-data.csv", encoding = "ISO-8859-1")
subject_df = main_df[['Document_Type', 'Path']]

#parse text from scanned files
def ocr_pdf(file):
    images = convert_from_path(file)
    ocr_list = [pytesseract.image_to_string(x) for x in images]
    ocr = ''
    return ocr.join(ocr_list)

#Run Tika Parser on Texts
def return_parsed(paths):
    try:
        return parser.from_file(paths, headers=headers)
    except:
        return 'path error'

#Extract Texts from Parsed Documents or OCR Documents
def return_texts(parsed, paths):
    if 'content' in parsed and parsed['content'] is not None:
        return parsed['content'] #extract 'content' from parsed texts
    else:
        try:
            return ocr_pdf(paths) #if no 'content' from tika parser, try OCRing the document
        except:
            return "no content"

#Function to remove whitespaces from Text
def remove_whitespace(text):
    return text.strip()

#Function to remove whitespaces between comma delimeters
def remove_comma_space(text):
    try:
        return text.replace(", ", ",")
    except:
        return text

#Parse Texts
subject_df = dd.from_pandas(subject_df, npartitions=5)
parsed = subject_df.apply(lambda row: return_parsed(row['Path']), axis = 1).compute()
subject_df['Parsed'] = parsed

#Extract Texts
texts =  subject_df.apply(lambda row: return_texts(row['Parsed'], row['Path']), axis = 1).compute()
subject_df['Texts'] = texts
subject_df = subject_df.compute()

#Remove whitespaces
Subject_Area =  subject_df.apply(lambda row: remove_comma_space(row['Subject_Area']), axis = 1)
subject_df['Subject_Area'] = Subject_Area

#Drop rows with nan values
isnan = subject_df[subject_df['Subject_Area'].isna() == True].index
subject_df.drop(isnan, inplace=True)

text_isnan = subject_df[subject_df['Texts'].isna() == True].index
subject_df.drop(text_isnan, inplace=True)

#Drop rows with no text content
no_content = subject_df[subject_df['Texts'] == 'no content'].index
subject_df.drop(no_content, inplace=True)
subject_df = subject_df.drop(columns = ['Unnamed: 0']) #only needed if imported from csv
subject_df.head()

#Import Necessary Modules
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.feature_selection import chi2
import pickle
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import LinearSVC

#Create Vectorizer Object
tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')

#Set y Values as list of sets
y = [set(i.split(',')) for i in subject_df.Subject_Area]

#Binarize y
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(y)

#Create ML features
X = subject_df.Texts

#Split data into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

#Vectorize Text
X_train = tfidf.fit_transform(X_train).toarray()
X_test = tfidf.transform(X_test).toarray()

def train_classifier(X_train, y_train, X_valid=None, y_valid=None, C=1.0, model='lr'):
    """
      X_train, y_train â€” training data
      
      return: trained classifier
      
    """
    
    if model=='lr':
        model = LogisticRegression(C=C, penalty='l1', dual=False, solver='liblinear')
        model = OneVsRestClassifier(model)
        model.fit(X_train, y_train)
    
    elif model=='svm':
        model = LinearSVC(C=C, penalty='l1', dual=False, loss='squared_hinge')
        model = OneVsRestClassifier(model)
        model.fit(X_train, y_train)
    
    elif model=='nbayes':
        model = MultinomialNB(alpha=1.0)
        model = OneVsRestClassifier(model)
        model.fit(X_train, y_train)
        
    elif model=='mlp':
        model = MLPClassifier(alpha=1.0)
        model = OneVsRestClassifier(model)
        model.fit(X_train, y_train)

    return model

#Train the Classifier
mlp = train_classifier(X_train, y_train, model = 'mlp')

#Check Model Score
print(mlp.score(X_test, y_test))

y_test_predicted_labels_tfidf = mlp.predict(X_test)

y_test_pred_inversed = mlb.inverse_transform(y_test_predicted_labels_tfidf)
y_test_inversed = mlb.inverse_transform(y_test)
for i in range(148):
    print('Title:\t{}\nTrue labels:\t{}\nPredicted labels:\t{}\n\n'.format(
        X_test[i],
        ','.join(y_test_inversed[i]),
        ','.join(y_test_pred_inversed[i])
    ))

# Predict the labels of the test data: y_pred
y_pred = test_me.predict(X_test)

# Generate the confusion matrix and classification report
#print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names = mlb.classes_))

#save trained model
model_save = r"\\esri-shelf\ESRIENVIROHUB\TaggerBot\subject-model.sav"
pickle.dump(mlp, open(model_save, 'wb'))

#Open saved trained model and test
loaded_model = pickle.load(open(model_save, 'rb'))
accuracy = loaded_model.score(X_test, y_test)
print(accuracy)